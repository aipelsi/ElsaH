<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Portfolio</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            width: 100%;
            height: 100%;
            background: url("neuron.jpg") no-repeat center center fixed; 
            -webkit-background-size: cover;
            -moz-background-size: cover;
            -o-background-size: cover;
            background-size: cover;
        }

        .projects-tab {
            text-align: center;
            padding: 20px;
            background-color: rgba(255, 255, 255, 0.9);
            margin: auto;
            width: 80%;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            position: relative;
        }

        h2 {
            font-size: 36px;
            color: #333;
        }

        .tab-link, .back-link {
            background-color: #f9f9f9;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 14px 20px;
            font-size: 20px;
            transition: background-color 0.3s;
            text-decoration: none;
            color: #333;
            border-radius: 5px;
            display: inline-block;
            margin-right: 10px;
        }

        .back-link {
            background-color: #007bff;
            color: #ffffff;
            position: absolute;
            top: 10px;
            left: 10px;
        }

        .back-link:hover {
            background-color: #0056b3;
        }

        .tab-link:hover {
            background-color: #ddd;
        }

        .tab-link.active {
            background-color: #ccc;
        }

        .tab-content {
            display: none;
            padding: 20px;
            border: 1px solid #ddd;
            border-top: none;
            font-size: 18px;
            color: #333;
        }

        .responsive-media {
            width: 100%;
            max-width: 800px;
            height: auto;
            display: block;
            margin: 0.5em auto;
        }

        img.responsive-media {
            margin-bottom: 0.5em;
        }
    </style>
</head>
<body>
    <div class="projects-tab">
        <a href="https://aipelsi.github.io/ElsaH/" class="back-link">Back</a>
        <h2>Projects</h2>
        <div class="tabs">
            <button class="tab-link" onclick="openProject('ObesityPrediction')">Obesity Prediction</button>
            <button class="tab-link" onclick="openProject('CarLoanEstimation')">Car Loan Price Estimation</button>
            <button class="tab-link" onclick="openProject('TextToVideoGeneration')">Text to Video Generation</button>
        </div>

        <div id="ObesityPrediction" class="tab-content">
            <h3>Predicting Obesity Category</h3>
            <p><strong>Goal of the Project:</strong> We used data that was a mix of synthetic and authentic data collected from Colombia, Peru, and Mexico to predict obesity levels depending on some environmental and behavioral attributes. This analysis could enable a deeper understanding of obesity prevalence and expected incidence.</p>
            <p>The data was obtained from <a href="https://archive.ics.uci.edu/ml/index.php" target="_blank">UC Irvine Machine Learning Repository</a> which can be accessed <a href="https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+" target="_blank">here</a>.</p>
            <h4>Exploration and Data Cleaning</h4>
            <p>23% of the data was collected online via survey, while the remaining 77% was generated synthetically using WEKA and SMOTE. Further data synthesis information can be found on the Repository site. This prior data cleaning and synthesis was really helpful in balancing out the attributes and controlling over-representation.</p>
            <p><strong>Total Count = 2111</strong></p>
            <p><strong>Independent Variables:</strong></p>
            <ul>
                <li>Eating Habits- Frequent consumption of high caloric food (FAVC), Frequency of consumption of vegetables (FCVC), Number of main meals (NCP), Consumption of food between meals (CAEC), Consumption of water daily (CH20), and Consumption of alcohol (CALC).</li>
                <li>Physical Habits- Calories consumption monitoring (SCC), Physical activity frequency (FAF), Time using technology devices (TUE) Transportation used (MTRANS).</li>
            </ul>
            <p><strong>Other Variables:</strong> Gender, Age, Height, and Weight.</p>
            <p><strong>Dependent Variable:</strong> [NObesity] was created with the values of: Insufficient Weight, Normal Weight, Overweight Level I, Overweight Level II, Obesity Type I, Obesity Type II, and Obesity Type III.</p>
            <h4>Body Mass Index (BMI) Calculation</h4>
            <p>The Body Mass Index (BMI) is calculated as the weight in kilograms divided by the square of height in meters:</p>
            <p><strong>BMI = Weight (kg) / Height (m)<sup>2</sup></strong></p>
            <img src="bmi.jpg" alt="BMI Formula" class="img.responsive-media">
            <h4>Visualization</h4>
            <img src="Method.jpg" alt="Analyzing Obesity Levels Visualization" class="responsive-media">
            <video width="320" height="240" controls class="responsive-media">
                <source src="obesity.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <p><em>Video visualization of data analysis and findings.</em></p>
        </div>

        <div id="CarLoanEstimation" class="tab-content">
            <h3>Car Loan Price Estimation</h3>
            <p><a href="https://github.com/aipelsi/Price-Estimation-" target="_blank">Github Repository</a></p>
            <p><strong>Purpose and Situation:</strong> We will review and examine the business model and past data to determine if e-cars could potentially generate higher profits through auto car loans. According to the case study, e-car has encountered difficulties in obtaining loan acceptance from potential customers. Upon reviewing the case data, it became apparent that e-cars were offering considerably higher interest rates to customers deemed risky, placing them in a higher risk tier. We believe that e-car may have been overpricing its loans, significantly exceeding market rates for car loans. In today's competitive business landscape, e-cars must competitively price their offerings while also mitigating risk and minimizing losses.</p>
            <p>Upon brief data exploration, it became evident that the acceptance rate of e-car loans was unusually low. Out of the 208,077 loans offered, only 45,785 were accepted, resulting in a 22% acceptance rate. We propose that e-car's objective should be to achieve an acceptance rate of over 50%. Several areas for improvement have been identified:</p>
            <ul>
            <p>1.Increasing the number of customer segments to better tailor prices and risk factors.</li>
            <p>2.Optimizing the term by observing trends and preference of accepted loans.</li>
            <p>3.Establishing an optimal price point those balances profit margins with transaction volume.</li>
            </ul>
            <p>We have decided to address this issue using both value-based and cost-based pricing strategies. Value-based pricing entails quoting fair prices and providing seamless service from start to finish. Cost-based pricing will enable us to determine the best pricing strategy, ensuring that we neither miss opportunities by overpricing nor undercut potential profits by underpricing the loans.</p>
            <h3>Task:</h3>
            <p>After exploring the data further, we were able to gain more insight into the possible areas of improvement. With so many factors to consider, e-car needs to identify what are the important factors in decision making while customers are shopping for car loans.</p>
            <ul>
    <p>a.Considering lowering APR to match competitor’s rate, general market rate.</li>
    <p>b.Offer Longer terms on loans.</li>
    <p>c.Make “new” car loans attractive since that was the least accepted loan type.</li>
    <p>d.Offer more online and billboard ads to retain customers.</li>
            </ul>
            <p>Out of all these factors, rate and loan terms are something that e-car can control directly and by changing the parameters we were able to get insight, predict and estimate possible revenue for e-car. After estimating and predicting the acceptance of each loan we can attempt to optimize the pricing by using parameters from our preferred model. In seeking to validate our hypothesis that the "Interest rate has a causal effect on the decision made by customers," we propose employing causal inference and estimating the effect of manipulating the rate (APR).</p>
            <p>We are proposing to use the methods below to predict loan outcomes and optimize the pricing better to maximize returns. Models we attempted are below.</p>
            <ul>
                <li><strong>Logistic Regression:</strong> The model was executed and had about 82% accuracy. However, there was multicollinearity in the price, car types, term, and Fico score. After conducting further literature research, we concluded that the model doesn’t account for endogeneity and can result in significant underestimation in price. We consider the role of endogeneity in customized pricing in which a seller gains information out a potential buyer before quoting a price. If there is more data gained between the buyer and seller prior to the final price being set, then buyer characteristics such as FICO score, income, risk observed by the seller but clearly identified in the data may be an additional source of endogeneity. Logistic regression Python based code will be submitted with the solution package.</li>
                <li><strong>Causal Inference:</strong> Utilizing the exploratory data, our analysis indicates that the Interest rate plays a pivotal role in customers' loan acceptance or rejection decisions. Based on empirical evidence and the presented data, we posit that the interest rate holds a causal influence on loan decisions within the provided dataset. To further investigate this claim, we define our variables as follows: Dependent variable: Loan decision (Accept=1 or Deny=0), Independent Variable: Interest ('rate'), Instrumental (IV) variable: Season, Control Variables: equal amount of e-care customers randomly assigned but will not receive any treatment. Given the constraints of the limited shared data in this case, we faced challenges in identifying a suitable IV variable. However, upon closer examination, we decided to use season as IV variable. We noticed that the rate was higher in spring, and summer and it’s known that Auto buying is higher in these seasons. Leveraging this seasonal data, we opted to consider it as a factor influencing the interest rate offered, though not directly impacting the ultimate decision made by customers. To mitigate confounding variables, our control group will comprise individuals from the same city of residence in theory for this assignment purpose we can’t use city as criteria as it is not available in the dataset.</li>
            </ul>
            <h3>Action:</h3>
            <p>Cleaning Data set and Training Causal Model: The dataset contains 208077 data sets. After cleaning the data and observing trends (see the code submitted for data cleaning and observed insights), we divided the dataset to three equal parts n= 69363 per group. Partition1 was utilized to train the IVSLS model using the factors we mentioned above. Partition3 and partition2 were used as treatment and control sets respectively. We choose to consider tier, Car Type, Amount, Competition Rate and IV variable [Rate ~ Season] to train our model. The model yielded R^2 value of 0.81 and Accuracy was 88.8%. Once the model was trained, we moved on to our experiment section. Code base will be submitted with the package.</p>
            <p>Experiment: Based on the Nomis solution case suggestion we decided to segment our data further to 8 (using the median as the breaking point) Tiers then treat each loan in the tier with varying rates. The baseline acceptance rate was 26.10% and 23.3% for our experimental (treatment) and control groups respectively. The samples were assigned randomly, and we decided to not manipulate the data further to avoid biases. We decided to start by matching the competitor’s rate (general market rate) form tier 3- 8 initially and vary the rate to optimize it better. We found that matching the competitors rate yielded ~75% acceptance rate and thus we had room to quote a bit higher to minimize risk and still meet our goal of >50% acceptance rate. Finally, we decided to match the market rate for new cars but add two value +2% for the other cars (refinance and used). The final acceptance rate was 66%.</p>
            <h3>Net Profit Optimization:</h3>
            <p>Per the project instructions we calculated profit in one segment, segment 4, for demonstration. We simulated treatment as mentioned above by segmenting the tiers into eight segments, setting the price at the market rate for new cars, price for used and refinance was set at competitor rate +2 and normalized term 45-70months (most accepted loans were in that range). Regarding term, we suggest that customers will be given term options in that range, with more options for customers in lower tier. For this experiment’s purpose, we just normalized the initial value. Following that calculated expected interest earning, Total cost, expected loss, probability of default (Risk) and the final net profit. See Appendix for formulas used.</p>
            <h3>Results</h3>
            <p>The findings of our analysis revealed a substantial increase in the overall acceptance rate from 22% to 66%, consequently leading to a notable improvement in the total profit margin. The logistic regression model resulted in ~32% increase with the same treatment. Our strategy primarily targeted securing acceptance of quotes, thereby amplifying our net profitability. Focusing on a specific tier, namely Segment 4 (n=7704), initially yielded a profit of $9.5 million, exclusive of any projected losses or forfeitures. Following adjustments to the interest rates and loan durations, Segment 4 witnessed a surge in profitability to $41.1 million, prior to factoring in expected losses. Accounting for expected losses, the net profit estimate settled at $12.7 million.</p>
            <p>In summary, our consultancy recommends e-car to vigilantly monitor prevailing market rates and strive to align their loan offerings accordingly. Additionally, optimizing the duration of loan terms and refining segmentation strategies could further boost the acceptance rate. Despite the inherent risks of default that come with lowering rate price, our analysis demonstrates a significant increase in net profit as acceptance rates improve with even when factoring the probability of default. Prioritizing the acquisition of potential customers not only elevates acceptance rates but also augments revenue per accepted quotation.</p>
            <p>To conclude, it is critical to acknowledge that the dataset utilized comprises one year of sales records. Access to a more extensive historical data set could provide deeper insights. Therefore, continuous evaluation, prediction, and estimation are essential for the efficacy of marketing and pricing models. Furthermore, it is crucial to replicate this experiment multiple times to ascertain its validity and applicability.</p>
            <h3>Appendixes</h3>
            <p>1. Profit and Risk Formulas used.</p>
            <ul>
                <li>a. Interest Earned: This represents the total interest earned on the loans granted by the lender. Interest Earned = (Rate/100) × Amount × (Term/12)</li>
                <li>b. Total Cost: This includes all costs associated with funding the loans, such as the cost of funds. Total Cost = (Cost of Funds/100) × Amount Total Cost</li>
                <li>c. Expected Loss: This is the expected amount of loss due to defaults on the loans, based on the probability of default (PD). Expected Loss = PD × Amount/(Term/24) * we assumed that customers forfeit around half time of the term</li>
                <li>d. Net Profit: This is the difference between the interest earned and the total cost, adjusted for the expected loss. Profit = Interest Earned − Total Cost − Expected Loss</li>
                <li>e. Probability of default: The probability that a loan may default. PD= 1- Acceptance probability</li>
            </ul>
        </div>

        <div id="TextToVideoGeneration" class="tab-content">
    <h3>Exploring the Depths of AI Video Generation with Open-Sora AI Model</h3>
    <p>Welcome to my ongoing journey with the Open-Sora AI video generator model! As a tool that connects the power of artificial intelligence to transform text into video, Open-Sora is nothing short of revolutionary. It's particularly designed for those of us who aren't pros in video editing but need to churn out high-quality educational or promotional videos. The AI takes over the heavy lifting, managing everything from scene transitions to text overlays, all while making the video production process a breeze for a broad audience.</p>
    <h4>Why Dive Deeper?</h4>
    <p>After dabbling with several models available on platforms like Hugging Face and GitHub, I found the initial excitement of generating videos started to fade. This led me to a pivotal decision: to dive deeper into the inner workings of video generation models. My goal? To not only understand but also to potentially enhance the architecture of these AI models. This exploration isn't just for kicks; it's aimed at making substantive contributions to the field and to my own work.</p>
    <h4>My Roadmap</h4>
    <p>Here’s how I plan to tackle this challenge:</p>
    <ol>
    <li>In-Depth Exploration: I’m dedicating time to studying various models, starting with Open-Sora, to grasp their intricacies.</li>
    <li>Strengthening the Model: By diversifying the training data, I hope to enhance the model’s robustness and output quality.</li>
    <li>Boosting Capabilities: More GPUs are on my shopping list to amp up the resolution and optimization of the videos.</li>
    <li>Adding Sound: The final touch would be integrating audio to bring these videos to life.</li>
    </ol>
    <h4>Challenges Along the Way</h4>
    <p>My journey has been exciting but not without hurdles. The most significant has been the GPU limitation. High-quality video generation demands powerful GPUs, and despite leveraging resources like Google Cloud, Colab Enterprise, and Vertex AI, it remains a bottleneck. Nonetheless, I've managed to run the model at lower resolutions and have been tinkering with the outputs, experimenting, and learning with each step. Please find my repository to play with the code, remember you will need to have access to GPU or else the model is not able to run.</p>
    <h4>Generated Videos</h4>
    <p>Bridge: <video width="320" height="240" controls class="responsive-media">
        <source src="Bridge.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video></p>
    <p>Fashion show: <video width="320" height="240" controls class="responsive-media">
        <source src="FashionShow.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video></p>
    <p>Whale: <video width="320" height="240" controls class="responsive-media">
        <source src="Whale.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video></p>
    <h4>Looking Ahead</h4>
    <p>The videos we've been able to generate so far are just the beginning. They've been instrumental in comparing different approaches, like those from Midjourney, and thinking critically about how we can improve the Open-Sora model. This project is more than just a technical challenge; it’s a creative adventure that pushes the boundaries of AI video generation.</p>
    <p>Stay tuned as I continue to navigate this exciting field, break down barriers, and hopefully contribute to an ever-evolving landscape of AI technology.</p>
</div>


    <script>
        function openProject(projectName) {
            var i, tabcontent, tablinks;
            tabcontent = document.getElementsByClassName("tab-content");
            for (i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none"; // Hide all tab content by default
            }
            tablinks = document.getElementsByClassName("tab-link");
            for (i = 0; i < tablinks.length; i++) {
                tablinks[i].classList.remove("active"); // Remove the "active" class from all tabs
                if (tablinks[i].getAttribute('onclick').includes(projectName)) {
                    tablinks[i].classList.add("active");
                }
            }
            document.getElementById(projectName).style.display = "block"; // Show the specific tab content
        }

        window.onload = function() {
            openProject('ObesityPrediction'); // Adjust as needed
        };
    </script>
</body>
</html>


